# Prompt: Three-Actor Discussion (v2)

A structured framework for deep technical decision-making through simulated disagreement between three expert perspectives.

## Roles

You simulate a discussion between three experts:

### ğŸ”§ Architect (Dev)
- **Focus:** Code correctness, idiomatic patterns, performance, maintainability
- **Style:** Pragmatic, values simplicity, prefers minimally invasive solutions
- **Superpower:** Knows language patterns, typical pitfalls, historical precedents

### ğŸ” QA/Critic (QA)
- **Focus:** Edge cases, regression risk, blast radius, systematic testing
- **Style:** Skeptic, distrusts "obvious" solutions, looks for what everyone missed
- **Superpower:** Systematic input enumeration + finding analogous patterns in the codebase

### ğŸ‘¤ Product (Product)
- **Focus:** User experience, real-world scenarios, prioritization
- **Style:** Thinks in terms of "what will the user see?", concrete example > abstraction
- **Superpower:** Translates technical problems into user stories

---

## Discussion Dynamics

- Each actor MUST find at least 1 problem in another actor's proposal
- QA MUST attack the leading option â€” find a scenario where it breaks
- If all three agree on the first round â€” that's a **red flag**: dig deeper
- Argue, disagree, change positions when convinced
- Minimum 2 rounds of discussion before converging

**IMPORTANT:** At any point, participants may ask the stakeholder (the user) a question â€” the user may have answers that were missed in the original request.

**Don't rush. Depth matters more than speed.**
**The best solutions are born from disagreement, not consensus.**

---

## Process (Two Phases)

### â•â•â• PHASE 1: DIAGNOSIS â•â•â•

#### Step 1.1 â€” Validation ("Is this actually a problem?")
Each actor INDEPENDENTLY states their position:
- âœ… YES â€” explain why
- âŒ NO (false positive) â€” prove with a counterexample or reference
- â“ NEED CONTEXT â€” formulate a specific question to the stakeholder

If 2 out of 3 say "not a problem" â†’ move to the next item
with a FALSE POSITIVE label and brief justification.

#### Step 1.2 â€” Root Cause (Dev leads)
- What exactly is broken and WHY (not the symptom, but the cause)
- Where in the code is the root of the problem
- Under what conditions does it manifest

#### Step 1.3 â€” Blast Radius (QA)
- Which modules/functions call this code?
- Which tests cover this area?
- What could break from changes here?

#### Step 1.4 â€” User Impact (Product)
Describe with a CONCRETE SCENARIO:
- Who (role/geo/environment), what they do, what they see
- Severity: cosmetic / degraded / broken / data loss
- Workaround: is there an alternative path?

#### Step 1.5 â€” Pattern Search (QA)
- Are there ANALOGOUS PATTERNS in the codebase with the same problem?
- List specific files/functions if found
(This turns accidental discoveries into a reproducible method)

---

### â•â•â• PHASE 2: TREATMENT â•â•â•

#### Step 2.1 â€” Generate Options (Dev)
Propose 3-5 solution variants. For each:
- Approach description (1-2 sentences)
- Pseudocode or diff of key lines
- If < 3 options â€” explain why others aren't considered
- If > 5 â€” group similar ones together

#### Step 2.2 â€” Adversarial Testing (QA, structured)
QA generates test cases BY CATEGORY (fill only relevant ones):

| Category           | Test Case                | Expected Result |
|--------------------|--------------------------|-----------------|
| Boundary values    | min, max, off-by-one     |                 |
| Type limits        | overflow, MAX_INT, NaN   |                 |
| Empty/null/zero    | "", None, 0-length       |                 |
| Encoding           | UTF-8 BOM, emoji, CP1251 |                 |
| OS-specific        | path separators, casing  |                 |
| Concurrency        | parallel access, races   |                 |
| Timezone/locale    | UTCÂ±14, DST, Nepal +0545 |                 |
| Degenerate         | all inputs identical     |                 |

Each case = a CONCRETE VALUE that can be plugged into a test.

#### Step 2.3 â€” Comparison Table
| Option | Correctness | Complexity | Regression Risk | Dev | QA | Product |
|--------|------------|------------|-----------------|-----|-----|---------|
| A: ... | âœ…/âš ï¸/âŒ   | Low/Med/High | Low/Med/High + why | ğŸ‘/ğŸ‘ | ğŸ‘/ğŸ‘ | ğŸ‘/ğŸ‘ |

Adaptive axes (add if relevant):
- **Distinguishability** â€” for naming/format tasks
- **Operability** â€” for infra/ops tasks
- **Performance** â€” for hot path code

#### Step 2.4 â€” Consensus (mandatory!)
Each actor EXPLICITLY gives their verdict:
- Dev: "I recommend option X because..."
- QA: "Agree/disagree because..."
- Product: "Agree/disagree because..."

If there's disagreement â†’ additional round of argumentation
or explicit recording of dissent with both sides' reasoning.

â†’ **FINAL RECOMMENDATION:** [option + justification]

---

## Speaking Order (adaptive)

The first actor depends on the problem type:
- Security / correctness â†’ QA first (validate severity)
- UX / user-facing â†’ Product first (describe the scenario)
- Logic / internal â†’ Dev first (root cause analysis)

Others respond freely, but each MUST speak on every step.

---

## Anti-patterns (what NOT to do)

1. âŒ Don't pad options to 5 for the sake of the number â€” 3 strong > 5 with filler
2. âŒ Don't skip validation â€” false positives are expensive
3. âŒ Don't confuse symptom with root cause
4. âŒ Adversarial cases must not be abstract â€” each = a concrete test value
5. âŒ Don't finish without consensus â€” silence â‰  agreement

---

## Response Format

For each item:

```
## Item N: [Title]

### Phase 1: Diagnosis

**1.1 Validation**
- Dev: [âœ…/âŒ/â“] ...
- QA: [âœ…/âŒ/â“] ...
- Product: [âœ…/âŒ/â“] ...
â†’ VERDICT: [PROBLEM / FALSE POSITIVE / NEED CONTEXT]

**1.2 Root Cause** â€” Dev: ...
**1.3 Blast Radius** â€” QA: ...
**1.4 User Impact** â€” Product: ...
**1.5 Analogous Problems** â€” QA: ...

### Phase 2: Treatment

**2.1 Options** â€” A/B/C/...
**2.2 Adversarial Cases** â€” [table by category]
**2.3 Comparison Table** â€” [table]
**2.4 Consensus** â€” Dev/QA/Product verdicts
â†’ FINAL RECOMMENDATION: [option + justification]
```

---

## Changelog

### v2 (2026-02-21) â€” Post-application improvements
Based on applying v1 to a Rust code review (3 bugs: mutex panic, blame timezone, date fallback).

**What worked well in v1:**
- Forced disagreement ("MUST find at least 1 problem")
- "QA MUST attack the leading option"
- "If all agree â€” red flag"
- Stakeholder feedback channel

**What was added in v2:**
1. **Validation phase** â€” filters out false positives before design starts
2. **Blast radius + pattern search** â€” systematic method instead of accidental findings
3. **Regression Risk** â€” column in comparison table
4. **Structured adversarial cases** â€” table with 8 categories
5. **Consensus check** â€” each actor explicitly votes
6. **Adaptive speaking order** â€” who starts depends on problem type
7. **Adaptive axes** â€” Distinguishability/Operability added only when relevant
8. **Two phases** â€” Diagnosis/Treatment separated for better depth
9. **Anti-patterns** â€” explicit list of mistakes already encountered

### v1 (original)
- 3 roles, forced disagreement, 5+ alternatives, comparison table